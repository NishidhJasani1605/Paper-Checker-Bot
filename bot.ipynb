{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9234850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>,\n",
       " <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=5512x7778>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "pdf_path = \"2.pdf\"\n",
    "images = convert_from_path(pdf_path)  # Each page = 1 image\n",
    "images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8806ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: pdf_pages\\page_1.jpg\n",
      "Saved: pdf_pages\\page_2.jpg\n",
      "Saved: pdf_pages\\page_3.jpg\n",
      "Saved: pdf_pages\\page_4.jpg\n",
      "Saved: pdf_pages\\page_5.jpg\n",
      "Saved: pdf_pages\\page_6.jpg\n",
      "Saved: pdf_pages\\page_7.jpg\n",
      "Saved: pdf_pages\\page_8.jpg\n",
      "Saved: pdf_pages\\page_9.jpg\n",
      "Saved: pdf_pages\\page_10.jpg\n",
      "Saved: pdf_pages\\page_11.jpg\n",
      "Saved: pdf_pages\\page_12.jpg\n",
      "Saved: pdf_pages\\page_13.jpg\n",
      "Saved: pdf_pages\\page_14.jpg\n",
      "Saved: pdf_pages\\page_15.jpg\n",
      "Saved: pdf_pages\\page_16.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"pdf_pages\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save images to disk\n",
    "for i, img in enumerate(images):\n",
    "    img_path = os.path.join(output_dir, f\"page_{i+1}.jpg\")\n",
    "    img.save(img_path, \"JPEG\")\n",
    "    print(f\"Saved: {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4956373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the images\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image, is_handwritten=True):\n",
    "    \"\"\"\n",
    "    Improved adaptive preprocessing for handwritten images\n",
    "    with uneven shadows or lighting.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "\n",
    "    # Apply light Gaussian blur\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Adaptive thresholding (block size and C adjusted)\n",
    "    adaptive = cv2.adaptiveThreshold(\n",
    "        blurred, 255,\n",
    "        cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "        cv2.THRESH_BINARY_INV,  # Invert to make text white on black (for optional cleaning)\n",
    "        35, 15  # Larger block size + higher constant to adapt to shadows\n",
    "    )\n",
    "\n",
    "    # Invert back to black text on white\n",
    "    adaptive = cv2.bitwise_not(adaptive)\n",
    "\n",
    "    # Light morphological close to connect letters (optional)\n",
    "    if is_handwritten:\n",
    "        kernel = np.ones((1, 1), np.uint8)\n",
    "        processed = cv2.morphologyEx(adaptive, cv2.MORPH_CLOSE, kernel)\n",
    "        return processed\n",
    "\n",
    "    return adaptive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3725813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255,   0, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "preprocessed_images = []\n",
    "\n",
    "for pil_image in images:\n",
    "    # Convert PIL to OpenCV image\n",
    "    cv_img = np.array(pil_image)\n",
    "    cv_img = cv_img[:, :, ::-1].copy()  # RGB to BGR\n",
    "\n",
    "    # Preprocess with handwritten mode\n",
    "    processed = preprocess_image(cv_img, is_handwritten=True)\n",
    "    \n",
    "    preprocessed_images.append(processed)\n",
    "\n",
    "preprocessed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240b98ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: preprocessed_pages\\page_1.png\n",
      "Saved: preprocessed_pages\\page_2.png\n",
      "Saved: preprocessed_pages\\page_3.png\n",
      "Saved: preprocessed_pages\\page_4.png\n",
      "Saved: preprocessed_pages\\page_5.png\n",
      "Saved: preprocessed_pages\\page_6.png\n",
      "Saved: preprocessed_pages\\page_7.png\n",
      "Saved: preprocessed_pages\\page_8.png\n",
      "Saved: preprocessed_pages\\page_9.png\n",
      "Saved: preprocessed_pages\\page_10.png\n",
      "Saved: preprocessed_pages\\page_11.png\n",
      "Saved: preprocessed_pages\\page_12.png\n",
      "Saved: preprocessed_pages\\page_13.png\n",
      "Saved: preprocessed_pages\\page_14.png\n",
      "Saved: preprocessed_pages\\page_15.png\n",
      "Saved: preprocessed_pages\\page_16.png\n"
     ]
    }
   ],
   "source": [
    "# Final processed images to perform ocr\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Create output folder if it doesn't exist\n",
    "output_dir = \"preprocessed_pages\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 2: Save each image to that folder\n",
    "for i, img in enumerate(preprocessed_images):\n",
    "    pil_img = Image.fromarray(img)  # Convert NumPy array to PIL image\n",
    "    img_path = os.path.join(output_dir, f\"page_{i+1}.png\")\n",
    "    pil_img.save(img_path)\n",
    "    print(f\"Saved: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9539b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Gemini AI configured successfully.\n",
      "--- E-Paper Checking Bot Workflow Initialized ---\n",
      "\n",
      "--- STEP 1: Locating Answer Sheet Images ---\n",
      " -> Found 16 answer pages to process.\n",
      "\n",
      "--- STEP 2: Preparing files for AI processing ---\n",
      " -> Uploading question paper: 2 - question.pdf\n",
      " -> Uploading answer page: page_1.png\n",
      " -> Uploading answer page: page_2.png\n",
      " -> Uploading answer page: page_3.png\n",
      " -> Uploading answer page: page_4.png\n",
      " -> Uploading answer page: page_5.png\n",
      " -> Uploading answer page: page_6.png\n",
      " -> Uploading answer page: page_7.png\n",
      " -> Uploading answer page: page_8.png\n",
      " -> Uploading answer page: page_9.png\n",
      " -> Uploading answer page: page_10.png\n",
      " -> Uploading answer page: page_11.png\n",
      " -> Uploading answer page: page_12.png\n",
      " -> Uploading answer page: page_13.png\n",
      " -> Uploading answer page: page_14.png\n",
      " -> Uploading answer page: page_15.png\n",
      " -> Uploading answer page: page_16.png\n",
      "\n",
      "--- STEP 3: Executing single-pass AI evaluation ---\n",
      " -> This may take a few moments as the AI processes all documents at once...\n",
      "\n",
      "--- STEP 4: Finalizing and Saving Output ---\n",
      "[SUCCESS] Final file created with 50 question-answer pairs.\n",
      " -> Output saved to: Student_QNA\\student_final_verified_qna.json\n",
      "\n",
      "âœ… Workflow complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Folder and File Paths ---\n",
    "# Folder containing the preprocessed images of the answer sheets\n",
    "PREPROCESSED_ANSWERS_FOLDER = \"preprocessed_pages\"\n",
    "# Path to the PDF file containing the questions\n",
    "QUESTION_PDF_PATH = \"2 - question.pdf\"\n",
    "# Folder for the final output\n",
    "FINAL_OUTPUT_FOLDER = \"Student_QNA\"\n",
    "# The final structured output file\n",
    "FINAL_JSON_OUTPUT_FILE = os.path.join(FINAL_OUTPUT_FOLDER, \"student_final_verified_qna.json\")\n",
    "\n",
    "# --- AI Configuration ---\n",
    "# Configure Gemini API using the correct environment variable name\n",
    "try:\n",
    "    # IMPORTANT: The environment variable should be GEMINI_API_KEY\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    print(\"[âœ“] Gemini AI configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"[FATAL ERROR] Failed to configure Gemini. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: THE \"SINGLE-PASS\" MASTER PROMPT\n",
    "# ==============================================================================\n",
    "\n",
    "MASTER_QNA_PROMPT = \"\"\"\n",
    "You are an expert examination evaluator AI. Your task is to accurately match questions from a provided question paper with their corresponding handwritten answers from a set of images and generate a single, structured JSON output.\n",
    "\n",
    "You will be given:\n",
    "1.  A PDF file containing all the exam questions (`question_paper.pdf`).\n",
    "2.  A series of images (`answer_page_X.png`) containing the student's handwritten answers.\n",
    "\n",
    "**Your process must be:**\n",
    "\n",
    "1.  **Iterate Through Questions:** Systematically go through the question paper, question by question, from start to finish.\n",
    "2.  **Locate the Answer:** For each question (e.g., \"Question 6\"), locate the student's corresponding handwritten answer in the provided images. Use question numbers (e.g., \"Ans-6\", \"Question-6\") as your primary guide.\n",
    "3.  **Transcribe Accurately:** Transcribe the handwritten answer text *exactly* as it appears, preserving all original wording, spelling, and grammar. Do not correct anything.\n",
    "4.  **Handle Unanswered Questions:** If you cannot find a corresponding answer for a question, you MUST explicitly mark it as \"Not Answered\". Do not skip it.\n",
    "5.  **Ignore Crossed-Out Text:** If any text is visibly struck through or crossed out, ignore it completely and do not include it in the transcription.\n",
    "\n",
    "**MANDATORY OUTPUT FORMAT:**\n",
    "\n",
    "-   Your final output must be a single, well-formed JSON array `[...]`.\n",
    "-   Each element in the array must be a JSON object `{...}` representing one question.\n",
    "-   Each object must contain these four keys:\n",
    "    -   `\"question_number\"`: The specific number of the question (e.g., \"1 (i)\", \"6\", \"12\").\n",
    "    -   `\"question_text\"`: The full text of the question.\n",
    "    -   `\"answer_text\"`: The student's transcribed answer. If not answered, this must be the string \"Not Answered\".\n",
    "    -   `\"status\"`: A status string, either \"Answered\" or \"Not Answered\".\n",
    "\n",
    "**Example JSON Object:**\n",
    "{\n",
    "    \"question_number\": \"12\",\n",
    "    \"question_text\": \"What is Barter System?\",\n",
    "    \"answer_text\": \"Bartering is the direct exchange of one goods with another goods without the use of money For eg for the services of Carpenter or blacksmith of he is given quintal of wheat then it is bartering.\",\n",
    "    \"status\": \"Answered\"\n",
    "}\n",
    "\n",
    "Do not add any text, notes, or explanations outside of the final JSON array.\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: CORE LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_structured_qna(question_pdf_path, answer_image_paths):\n",
    "    \"\"\"\n",
    "    Performs the main Q&A matching and verification in a single AI call.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- STEP 2: Preparing files for AI processing ---\")\n",
    "    try:\n",
    "        # Prepare the list of files to send to the AI\n",
    "        # The first item is the master prompt\n",
    "        prompt_parts = [MASTER_QNA_PROMPT]\n",
    "\n",
    "        # Add the question paper PDF\n",
    "        print(f\" -> Uploading question paper: {question_pdf_path}\")\n",
    "        prompt_parts.append(genai.upload_file(path=question_pdf_path, display_name=\"question_paper.pdf\"))\n",
    "\n",
    "        # Add all the answer page images\n",
    "        for i, img_path in enumerate(answer_image_paths):\n",
    "            print(f\" -> Uploading answer page: {os.path.basename(img_path)}\")\n",
    "            prompt_parts.append(genai.upload_file(path=img_path, display_name=f\"answer_page_{i+1}.png\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL ERROR] Failed to upload files for processing. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n--- STEP 3: Executing single-pass AI evaluation ---\")\n",
    "    print(\" -> This may take a few moments as the AI processes all documents at once...\")\n",
    "    try:\n",
    "        # Make the single, powerful API call\n",
    "        response = model.generate_content(prompt_parts)\n",
    "\n",
    "        # Extract the JSON from the response\n",
    "        json_match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response.text)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            print(\"[WARNING] Could not find a JSON block in the AI's response. Trying to parse the whole text.\")\n",
    "            # Fallback for when the AI forgets the markdown block\n",
    "            return json.loads(response.text)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"[ERROR] Failed to parse JSON from the AI response. The response may be malformed.\")\n",
    "        print(\"--- AI Response Text ---\")\n",
    "        print(response.text)\n",
    "        print(\"------------------------\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An unexpected error occurred during AI generation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: MAIN EXECUTION WORKFLOW\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- E-Paper Checking Bot Workflow Initialized ---\")\n",
    "    os.makedirs(FINAL_OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "    print(\"\\n--- STEP 1: Locating Answer Sheet Images ---\")\n",
    "    if not os.path.exists(PREPROCESSED_ANSWERS_FOLDER):\n",
    "        print(f\"[FATAL ERROR] Answer folder not found: '{PREPROCESSED_ANSWERS_FOLDER}'\")\n",
    "        exit()\n",
    "\n",
    "    # Get a sorted list of all answer image files\n",
    "    answer_files = sorted(\n",
    "        [os.path.join(PREPROCESSED_ANSWERS_FOLDER, f) for f in os.listdir(PREPROCESSED_ANSWERS_FOLDER) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))],\n",
    "        key=lambda x: int(re.search(r'\\d+', os.path.basename(x)).group())\n",
    "    )\n",
    "\n",
    "    if not answer_files:\n",
    "        print(f\"[FATAL ERROR] No image files found in '{PREPROCESSED_ANSWERS_FOLDER}'.\")\n",
    "        exit()\n",
    "    \n",
    "    print(f\" -> Found {len(answer_files)} answer pages to process.\")\n",
    "\n",
    "    # Generate the final structured data\n",
    "    final_qna_data = generate_structured_qna(QUESTION_PDF_PATH, answer_files)\n",
    "\n",
    "    print(\"\\n--- STEP 4: Finalizing and Saving Output ---\")\n",
    "    if final_qna_data and isinstance(final_qna_data, list):\n",
    "        with open(FINAL_JSON_OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            # Save as a pretty-printed JSON file, which is ideal for machine processing\n",
    "            json.dump(final_qna_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"[SUCCESS] Final file created with {len(final_qna_data)} question-answer pairs.\")\n",
    "        print(f\" -> Output saved to: {FINAL_JSON_OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"[FAILURE] No valid Q&A data was generated. The output file was not created.\")\n",
    "\n",
    "    print(\"\\nâœ… Workflow complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9372cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nishi\\anaconda3\\envs\\All_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Gemini AI configured successfully.\n",
      "--- Official Q&A JSON Generator Workflow Started ---\n",
      "\n",
      "--- STEP 1: Verifying PDF file paths ---\n",
      " -> All required PDF files found.\n",
      "\n",
      "--- STEP 2: Preparing files for AI processing ---\n",
      "\n",
      "--- STEP 3: Starting AI extraction ---\n",
      " -> This may take a moment...\n",
      "\n",
      "--- STEP 4: Finalizing and saving the output ---\n",
      "[SUCCESS] Final file created with 50 question-answer pairs.\n",
      " -> Output saved to: Original_Answer\\original_answer.json\n",
      "\n",
      "âœ… Workflow complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- File Paths ---\n",
    "# Path to the question paper PDF\n",
    "QUESTION_PDF_PATH = \"2 - question.pdf\"\n",
    "# Path to the model answer key PDF\n",
    "OFFICIAL_ANSWER_PDF_PATH = \"2 - answer.pdf\"\n",
    "# Output folder name\n",
    "FINAL_OUTPUT_FOLDER = \"Original_Answer\"\n",
    "# Path to the final JSON output file\n",
    "FINAL_JSON_OUTPUT_FILE = os.path.join(FINAL_OUTPUT_FOLDER, \"original_answer.json\")\n",
    "\n",
    "# --- AI Configuration ---\n",
    "# Configure your Gemini API key here\n",
    "try:\n",
    "    # Get API key from environment variable\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    print(\"[âœ“] Gemini AI configured successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"[FATAL ERROR] Failed to configure Gemini. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: THE MASTER PROMPT (***RE-ENGINEERED FOR COMPLETENESS***)\n",
    "# ==============================================================================\n",
    "\n",
    "MASTER_PROMPT = \"\"\"\n",
    "You are a meticulous and highly precise data extraction AI. Your primary directive is to create a **COMPLETE** and **VERBATIM** JSON representation of the provided question paper and its official answers. You *must* process every question from start to finish without any omissions.\n",
    "\n",
    "You will be given:\n",
    "1.  `question_paper.pdf`: Contains exam questions.\n",
    "2.  `official_answer_key.pdf`: Contains the official answers.\n",
    "\n",
    "**CRITICAL RULES FOR EXECUTION:**\n",
    "\n",
    "1.  ***ABSOLUTE COMPLETENESS & VERIFICATION***:\n",
    "    -   You **MUST** process **ALL 23 questions** from the `question_paper.pdf`, from Question 1 to Question 23.\n",
    "    -   Do not stop early under any circumstances.\n",
    "    -   **Before finishing, you must perform a final self-check to ensure all 23 questions and their sub-parts are present in your final JSON output.**\n",
    "\n",
    "2.  ***VERBATIM (EXACT) EXTRACTION***:\n",
    "    -   All extracted text must be a *character-for-character copy*.\n",
    "    -   Do not translate, summarize, rephrase, or alter any text.\n",
    "    -   The original languages in the source documents (e.g., English and Hindi) must be preserved perfectly.\n",
    "\n",
    "3.  ***QUESTION TEXT SPECIFICATIONS***:\n",
    "    -   For each question number, extract the full **English version** of the question text.\n",
    "    -   If a question has multiple-choice options (e.g., A, B, C, D), you **MUST** include those options as part of the `question_text`.\n",
    "\n",
    "4.  ***\"OR\" (à¤…à¤¥à¤µà¤¾) QUESTION HANDLING***:\n",
    "    -   Many questions have an 'OR' (à¤…à¤¥à¤µà¤¾) option. For these, your `question_text` **MUST** include the text for **BOTH** the main question and the 'OR' question.\n",
    "    -   The `official_answer_text` should be the single answer provided in the answer key, which may correspond to either the main question or the 'OR' part. Extract whichever answer is present.\n",
    "\n",
    "**MANDATORY OUTPUT FORMAT:**\n",
    "\n",
    "-   Your final output must be a single, well-formed JSON array `[...]`.\n",
    "-   Each object must contain these three keys:\n",
    "    -   `\"question_number\"`: The specific number (e.g., \"1 (i)\", \"6\", \"23\").\n",
    "    -   `\"question_text\"`: The full English text of the question (and its 'OR' part, if present), copied verbatim.\n",
    "    -   `\"official_answer_text\"`: The full text of the corresponding answer from the answer key, copied verbatim in its original language.\n",
    "\n",
    "**Example for a Multiple-Choice Question:**\n",
    "{\n",
    "    \"question_number\": \"1 (i)\",\n",
    "    \"question_text\": \"Study of a firm, an industries, price of a good is done under -\\n(A) Macro economics\\n(B) Micro economics\\n(C) Both (A and B)\\n(D) National income\",\n",
    "    \"official_answer_text\": \"(à¤¬) à¤µà¥à¤¯à¤·à¥à¤Ÿà¤¿ à¤…à¤°à¥à¤¥à¤¶à¤¾à¤¸à¥à¤¤à¥à¤°\"\n",
    "}\n",
    "\n",
    "**Example for a Question with an \"OR\" part:**\n",
    "{\n",
    "    \"question_number\": \"6\",\n",
    "    \"question_text\": \"What are the central problems of an economy?\\nOR\\nDefine Economic Problem.\",\n",
    "    \"official_answer_text\": \"(i) à¤•à¤¿à¤¨ à¤µà¤¸à¥à¤¤à¥à¤“à¤‚ à¤•à¤¾ à¤•à¤¿à¤¤à¤¨à¥€ à¤®à¤¾à¤¤à¥à¤°à¤¾ à¤®à¥‡à¤‚ à¤‰à¤¤à¥à¤ªà¤¾à¤¦à¤¨ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¯à¥‡à¥¤ (ii) à¤µà¤¸à¥à¤¤à¥à¤“à¤‚ à¤•à¤¾ à¤‰à¤¤à¥à¤ªà¤¾à¤¦à¤¨ à¤•à¤¿à¤¸à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¯à¥‡à¥¤ (iii) à¤µà¤¸à¥à¤¤à¥à¤“à¤‚ à¤•à¥‡ à¤‰à¤¤à¥à¤ªà¤¾à¤¦à¤¨ à¤•à¤¾ à¤¢à¤‚à¤— à¤•à¥à¤¯à¤¾ à¤¹à¥‹?\"\n",
    "}\n",
    "\n",
    "Do not add any text, notes, or explanations outside of the final JSON array.\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: CORE LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_official_qna_json(question_pdf, answer_pdf):\n",
    "    \"\"\"\n",
    "    Uses AI to extract and combine questions and official answers.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- STEP 2: Preparing files for AI processing ---\")\n",
    "    try:\n",
    "        # List of prompt and files to send to the AI\n",
    "        prompt_parts = [\n",
    "            MASTER_PROMPT,\n",
    "            genai.upload_file(path=question_pdf, display_name=\"question_paper.pdf\"),\n",
    "            genai.upload_file(path=answer_pdf, display_name=\"official_answer_key.pdf\")\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"[FATAL ERROR] Failed to upload files. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n--- STEP 3: Starting AI extraction ---\")\n",
    "    print(\" -> This may take a moment...\")\n",
    "    try:\n",
    "        # Send the request to the AI\n",
    "        response = model.generate_content(prompt_parts)\n",
    "\n",
    "        # Extract the JSON from the response\n",
    "        json_match = re.search(r'```json\\s*([\\s\\S]*?)\\s*```', response.text)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            print(\"[WARNING] No JSON block found in AI response. Attempting to parse the full text.\")\n",
    "            return json.loads(response.text)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"[ERROR] Failed to parse JSON from AI response. The response may be malformed.\")\n",
    "        print(\"--- AI Response Text ---\")\n",
    "        print(response.text)\n",
    "        print(\"------------------------\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An unexpected error occurred during AI generation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: MAIN EXECUTION WORKFLOW\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Official Q&A JSON Generator Workflow Started ---\")\n",
    "    os.makedirs(FINAL_OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "    print(\"\\n--- STEP 1: Verifying PDF file paths ---\")\n",
    "    if not os.path.exists(QUESTION_PDF_PATH) or not os.path.exists(OFFICIAL_ANSWER_PDF_PATH):\n",
    "        print(f\"[FATAL ERROR] Please ensure that '{QUESTION_PDF_PATH}' and '{OFFICIAL_ANSWER_PDF_PATH}' exist.\")\n",
    "        exit()\n",
    "\n",
    "    print(\" -> All required PDF files found.\")\n",
    "\n",
    "    # Generate the final structured data\n",
    "    final_qna_data = generate_official_qna_json(QUESTION_PDF_PATH, OFFICIAL_ANSWER_PDF_PATH)\n",
    "\n",
    "    print(\"\\n--- STEP 4: Finalizing and saving the output ---\")\n",
    "    if final_qna_data and isinstance(final_qna_data, list):\n",
    "        with open(FINAL_JSON_OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_qna_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"[SUCCESS] Final file created with {len(final_qna_data)} question-answer pairs.\")\n",
    "        print(f\" -> Output saved to: {FINAL_JSON_OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"[FAILURE] No valid Q&A data was generated. The output file was not created.\")\n",
    "\n",
    "    print(\"\\nâœ… Workflow complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a486e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Gemini AI configured successfully for evaluation.\n",
      "--- Paper Checker Bot Initialized ---\n",
      "-> Loading official answers from: Original_Answer\\original_answer.json\n",
      "-> Loading student answers from: Student_QNA\\student_final_verified_qna.json\n",
      "-> Merging and preparing data for evaluation...\n",
      "\n",
      "--- Starting Automated Evaluation ---\n",
      "-> Evaluating question 1/50: '1 (i)'... SCORE: 0%\n",
      "-> Evaluating question 2/50: '1 (ii)'... SCORE: 0%\n",
      "-> Evaluating question 3/50: '1 (iii)'... SCORE: 0%\n",
      "-> Evaluating question 4/50: '1 (iv)'... SCORE: 100%\n",
      "-> Evaluating question 5/50: '1 (v)'... SCORE: 100%\n",
      "-> Evaluating question 6/50: '1 (vi)'... SCORE: 0%\n",
      "-> Evaluating question 7/50: '2 (i)'... SCORE: 100%\n",
      "-> Evaluating question 8/50: '2 (ii)'... SCORE: 100%\n",
      "-> Evaluating question 9/50: '2 (iii)'... SCORE: 100%\n",
      "-> Evaluating question 10/50: '2 (iv)'... SCORE: 0%\n",
      "-> Evaluating question 11/50: '2 (v)'... SCORE: 0%\n",
      "-> Evaluating question 12/50: '2 (vi)'... SCORE: 0%\n",
      "-> Evaluating question 13/50: '2 (vii)'... SCORE: 100%\n",
      "-> Evaluating question 14/50: '3 (i)'... SCORE: 0%\n",
      "-> Evaluating question 15/50: '3 (ii)'... SCORE: 100%\n",
      "-> Evaluating question 16/50: '3 (iii)'... SCORE: 100%\n",
      "-> Evaluating question 17/50: '3 (iv)'... SCORE: 0%\n",
      "-> Evaluating question 18/50: '3 (v)'... SCORE: 100%\n",
      "-> Evaluating question 19/50: '3 (vi)'... SCORE: 0%\n",
      "-> Evaluating question 20/50: '4 (i)'... SCORE: 0%\n",
      "-> Evaluating question 21/50: '4 (ii)'... SCORE: 0%\n",
      "-> Evaluating question 22/50: '4 (iii)'... SCORE: 0%\n",
      "-> Evaluating question 23/50: '4 (iv)'... SCORE: 0%\n",
      "-> Evaluating question 24/50: '4 (v)'... SCORE: 0%\n",
      "-> Evaluating question 25/50: '4 (vi)'... SCORE: 0%\n",
      "-> Evaluating question 26/50: '5 (i)'... SCORE: 100%\n",
      "-> Evaluating question 27/50: '5 (ii)'... SCORE: 100%\n",
      "-> Evaluating question 28/50: '5 (iii)'... SCORE: 100%\n",
      "-> Evaluating question 29/50: '5 (iv)'... SCORE: 0%\n",
      "-> Evaluating question 30/50: '5 (v)'... SCORE: 100%\n",
      "-> Evaluating question 31/50: '5 (vi)'... SCORE: 100%\n",
      "-> Evaluating question 32/50: '5 (vii)'... SCORE: 100%\n",
      "-> Evaluating question 33/50: '6'... SCORE: 20%\n",
      "-> Evaluating question 34/50: '7'... SCORE: 80%\n",
      "-> Evaluating question 35/50: '8'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 36/50: '9'... SCORE: 0%\n",
      "-> Evaluating question 37/50: '10'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 38/50: '11'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 39/50: '12'... SCORE: 90%\n",
      "-> Evaluating question 40/50: '13'... SCORE: 0%\n",
      "-> Evaluating question 41/50: '14'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 42/50: '15'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 43/50: '16'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 44/50: '17'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 45/50: '18'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 46/50: '19'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 47/50: '20'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 48/50: '21'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 49/50: '22'... SKIPPED (Not Answered)\n",
      "-> Evaluating question 50/50: '23'... SKIPPED (Not Answered)\n",
      "\n",
      "--- Generating Evaluation Report ---\n",
      "[SUCCESS] Evaluation report saved to: Final_Evaluation\\evaluation_report.txt\n",
      "\n",
      "âœ… Evaluation and Report Generation Complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- File Paths ---\n",
    "# Path to the JSON file with official answers\n",
    "ORIGINAL_ANSWERS_PATH = os.path.join(\"Original_Answer\", \"original_answer.json\")\n",
    "# Path to the JSON file with the student's answers\n",
    "STUDENT_ANSWERS_PATH = os.path.join(\"Student_QNA\", \"student_final_verified_qna.json\")\n",
    "# Output folder for the final report\n",
    "EVALUATION_OUTPUT_FOLDER = \"Final_Evaluation\"\n",
    "# Path to the final text report file\n",
    "EVALUATION_REPORT_FILE = os.path.join(EVALUATION_OUTPUT_FOLDER, \"evaluation_report.txt\")\n",
    "\n",
    "# --- AI Configuration ---\n",
    "try:\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    # Configure the model for evaluation tasks\n",
    "    evaluation_model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    print(\"[âœ“] Gemini AI configured successfully for evaluation.\")\n",
    "except Exception as e:\n",
    "    print(f\"[FATAL ERROR] Failed to configure Gemini. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: AI EVALUATION PROMPT\n",
    "# ==============================================================================\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert, impartial examiner. Your task is to evaluate a student's answer against the official model answer and provide a score based on semantic correctness.\n",
    "\n",
    "**Context:**\n",
    "- The student's answer and the official answer may be in different languages (e.g., English and Hindi).\n",
    "- Your evaluation must be based on the *meaning and core concepts*, not just keyword matching.\n",
    "\n",
    "**Official Answer:**\n",
    "---\n",
    "{official_answer}\n",
    "---\n",
    "\n",
    "**Student's Answer:**\n",
    "---\n",
    "{student_answer}\n",
    "---\n",
    "\n",
    "**Your Task:**\n",
    "1.  Compare the student's answer to the official answer.\n",
    "2.  Provide a numerical score from 0 to 100, where 100 means the student's answer perfectly conveys the same meaning as the official answer, and 0 means it is completely incorrect or irrelevant.\n",
    "3.  Provide a brief, one-sentence justification for your score.\n",
    "\n",
    "**Mandatory Output Format:**\n",
    "You MUST return your response in a single line with the format: `score|justification`\n",
    "**Example 1:** `90|The student correctly explained the concept but missed one minor detail mentioned in the official answer.`\n",
    "**Example 2:** `0|The student's answer is factually incorrect and does not align with the official answer.`\n",
    "**Example 3:** `100|The student's answer perfectly matches the core concepts of the official answer.`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: CORE LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def load_and_merge_data(original_file, student_file):\n",
    "    \"\"\"Loads data from both JSON files and merges them for comparison.\"\"\"\n",
    "    print(f\"-> Loading official answers from: {original_file}\")\n",
    "    try:\n",
    "        with open(original_file, 'r', encoding='utf-8') as f:\n",
    "            original_data = {item['question_number']: item for item in json.load(f)}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Official answer file not found at '{original_file}'\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"-> Loading student answers from: {student_file}\")\n",
    "    try:\n",
    "        with open(student_file, 'r', encoding='utf-8') as f:\n",
    "            student_data = {item['question_number']: item for item in json.load(f)}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Student answer file not found at '{student_file}'\")\n",
    "        return None\n",
    "\n",
    "    merged_data = []\n",
    "    print(\"-> Merging and preparing data for evaluation...\")\n",
    "    for q_num, original_q in original_data.items():\n",
    "        student_q = student_data.get(q_num)\n",
    "        if student_q:\n",
    "            merged_data.append({\n",
    "                \"question_number\": q_num,\n",
    "                \"question_text\": original_q.get(\"question_text\", \"N/A\"),\n",
    "                \"official_answer\": original_q.get(\"official_answer_text\", \"\"),\n",
    "                \"student_answer\": student_q.get(\"answer_text\", \"\"),\n",
    "                \"status\": student_q.get(\"status\", \"Not Answered\")\n",
    "            })\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def evaluate_answers(data_to_evaluate):\n",
    "    \"\"\"Iterates through data and uses AI to score each answered question.\"\"\"\n",
    "    print(\"\\n--- Starting Automated Evaluation ---\")\n",
    "    evaluated_results = []\n",
    "    total_questions = len(data_to_evaluate)\n",
    "    \n",
    "    for i, item in enumerate(data_to_evaluate):\n",
    "        print(f\"-> Evaluating question {i+1}/{total_questions}: '{item['question_number']}'...\", end='')\n",
    "        if item[\"status\"] != \"Answered\" or not item[\"student_answer\"] or item[\"student_answer\"] == \"Not Answered\":\n",
    "            item[\"score\"] = 0\n",
    "            item[\"justification\"] = \"Question was not answered by the student.\"\n",
    "            print(\" SKIPPED (Not Answered)\")\n",
    "        else:\n",
    "            prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "                official_answer=item[\"official_answer\"],\n",
    "                student_answer=item[\"student_answer\"]\n",
    "            )\n",
    "            try:\n",
    "                response = evaluation_model.generate_content(prompt)\n",
    "                # Parse the response: score|justification\n",
    "                parts = response.text.strip().split('|')\n",
    "                if len(parts) == 2:\n",
    "                    item[\"score\"] = int(re.sub(r'[^0-9]', '', parts[0])) # Clean non-numeric chars\n",
    "                    item[\"justification\"] = parts[1].strip()\n",
    "                    print(f\" SCORE: {item['score']}%\")\n",
    "                else:\n",
    "                    raise ValueError(\"Response was not in the expected 'score|justification' format.\")\n",
    "            except Exception as e:\n",
    "                item[\"score\"] = -1  # Use -1 to indicate an evaluation error\n",
    "                item[\"justification\"] = f\"AI evaluation failed: {e}\"\n",
    "                print(f\" FAILED ({e})\")\n",
    "\n",
    "        evaluated_results.append(item)\n",
    "    return evaluated_results\n",
    "\n",
    "\n",
    "def generate_evaluation_report(results):\n",
    "    \"\"\"Generates a detailed text file report from the evaluation results.\"\"\"\n",
    "    print(\"\\n--- Generating Evaluation Report ---\")\n",
    "    os.makedirs(EVALUATION_OUTPUT_FOLDER, exist_ok=True)\n",
    "    \n",
    "    total_score = 0\n",
    "    answered_count = 0\n",
    "    \n",
    "    with open(EVALUATION_REPORT_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"======================================================================\\n\")\n",
    "        f.write(\"                 AUTOMATED ANSWER EVALUATION REPORT\\n\")\n",
    "        f.write(\"======================================================================\\n\\n\")\n",
    "\n",
    "        for item in results:\n",
    "            f.write(f\"--- Question {item['question_number']} ---\\n\")\n",
    "            f.write(f\"Status: {item['status']}\\n\")\n",
    "            f.write(f\"Score: {item['score']}%\\n\")\n",
    "            f.write(f\"Justification: {item['justification']}\\n\\n\")\n",
    "            f.write(\"ðŸ‘¤ Student's Answer:\\n\")\n",
    "            f.write(f\"{item['student_answer']}\\n\\n\")\n",
    "            f.write(\"ðŸ“š Official Answer:\\n\")\n",
    "            f.write(f\"{item['official_answer']}\\n\")\n",
    "            f.write(\"----------------------------------------------------------------------\\n\\n\")\n",
    "            \n",
    "            if item['status'] == 'Answered' and item['score'] != -1:\n",
    "                total_score += item['score']\n",
    "                answered_count += 1\n",
    "                \n",
    "        # Final Summary\n",
    "        average_score = (total_score / answered_count) if answered_count > 0 else 0\n",
    "        f.write(\"======================================================================\\n\")\n",
    "        f.write(\"                            FINAL SUMMARY\\n\")\n",
    "        f.write(\"======================================================================\\n\")\n",
    "        f.write(f\"Total Questions: {len(results)}\\n\")\n",
    "        f.write(f\"Questions Answered by Student: {answered_count}\\n\")\n",
    "        f.write(f\"Average Score on Answered Questions: {average_score:.2f}%\\n\")\n",
    "        f.write(\"======================================================================\\n\")\n",
    "    \n",
    "    print(f\"[SUCCESS] Evaluation report saved to: {EVALUATION_REPORT_FILE}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: MAIN EXECUTION WORKFLOW\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Paper Checker Bot Initialized ---\")\n",
    "    \n",
    "    # 1. Load and merge data from both JSON files\n",
    "    merged_data = load_and_merge_data(ORIGINAL_ANSWERS_PATH, STUDENT_ANSWERS_PATH)\n",
    "    \n",
    "    if merged_data:\n",
    "        # 2. Evaluate the answers using the AI model\n",
    "        final_results = evaluate_answers(merged_data)\n",
    "        \n",
    "        # 3. Generate a detailed report file\n",
    "        generate_evaluation_report(final_results)\n",
    "        \n",
    "        # NOTE: The interactive query mode has been removed from this script\n",
    "        # as per your request to separate the tasks.\n",
    "        \n",
    "        print(\"\\nâœ… Evaluation and Report Generation Complete.\")\n",
    "    else:\n",
    "        print(\"\\n[FAILURE] Could not proceed due to errors in loading data. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4368f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "All_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
